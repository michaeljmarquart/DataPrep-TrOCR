{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3dd3e42e",
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import shutil\n",
                "from collections import defaultdict\n",
                "from pathlib import Path\n",
                "from typing import Any, Dict, List, Tuple\n",
                "\n",
                "import cv2\n",
                "import easyocr\n",
                "import numpy as np\n",
                "import torch\n",
                "from numpy import ndarray\n",
                "from paddleocr import LayoutDetection, TextDetection\n",
                "from scipy.signal import find_peaks\n",
                "\n",
                "prep_root = Path.cwd()\n",
                "IMAGE_PATH = prep_root / \"images\" / \"example.jpg\"\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "has_gpu = torch.cuda.is_available()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1182a728",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1: Deskew manually (optional)\n",
                "\n",
                "\n",
                "def deskew(image_path: Path, angle: float) -> None:\n",
                "    image_path = Path(image_path)\n",
                "    working_dir = image_path.parent / image_path.stem\n",
                "    working_dir.mkdir(exist_ok=True)\n",
                "    working_image = working_dir / image_path.name\n",
                "\n",
                "    source = image_path\n",
                "    img = cv2.imread(str(source), cv2.IMREAD_UNCHANGED)\n",
                "\n",
                "    h, w = img.shape[:2]\n",
                "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
                "    deskewed = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
                "\n",
                "    cv2.imwrite(str(working_image), deskewed)\n",
                "    print(f\"Rotated by {angle}° -> {working_image}\")\n",
                "\n",
                "\n",
                "deskew(IMAGE_PATH, 0.1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8ff9b090",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2: Crop to table region (PaddleOCR handles dark scan artifacts better than alternatives)\n",
                "\n",
                "# Parameters are found using 'line_detection.py'\n",
                "LINE_DETECTION_CONFIG = {\n",
                "    \"blur_kernel\": 5,\n",
                "    \"canny_low\": 50,\n",
                "    \"canny_high\": 150,\n",
                "    \"morph_kernel\": 3,\n",
                "    \"hough_threshold\": 100,\n",
                "    \"min_line_length\": 2000,\n",
                "    \"max_line_gap\": 1,\n",
                "    \"strip_height\": 21,\n",
                "    \"density_threshold\": 2.8,\n",
                "}\n",
                "\n",
                "\n",
                "def _find_table_top_edge(image: ndarray, vis: ndarray, config: Dict[str, Any], current_y1: int) -> int:\n",
                "    \"\"\"\n",
                "    Refines the top edge of the table using Hough lines and edge density.\n",
                "    \"\"\"\n",
                "    h, w = image.shape[:2]\n",
                "\n",
                "    blur_k = config[\"blur_kernel\"]\n",
                "    morph_k = config[\"morph_kernel\"]\n",
                "\n",
                "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
                "    processed = cv2.GaussianBlur(gray, (blur_k, blur_k), 0)\n",
                "    edges = cv2.Canny(processed, config[\"canny_low\"], config[\"canny_high\"])\n",
                "\n",
                "    kernel = np.ones((morph_k, morph_k), np.uint8)\n",
                "    edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
                "\n",
                "    detections = []\n",
                "\n",
                "    # Method A: Hough Lines\n",
                "    lines = cv2.HoughLinesP(\n",
                "        edges,\n",
                "        1,\n",
                "        np.pi / 180,\n",
                "        config[\"hough_threshold\"],\n",
                "        minLineLength=config[\"min_line_length\"],\n",
                "        maxLineGap=config[\"max_line_gap\"],\n",
                "    )\n",
                "\n",
                "    if lines is not None:\n",
                "        filtered_lines = []\n",
                "        for line in lines:\n",
                "            lx1, ly1, lx2, ly2 = line[0]\n",
                "            angle = abs(np.arctan2(ly2 - ly1, lx2 - lx1) * 180 / np.pi)\n",
                "\n",
                "            # Filter: Keep only horizontal lines (0° or 180° range)\n",
                "            if angle <= 15 or angle >= 165:\n",
                "                filtered_lines.append(line)\n",
                "\n",
                "        for line in filtered_lines:\n",
                "            lx1, ly1, lx2, ly2 = line[0]\n",
                "\n",
                "            cv2.line(vis, (lx1, ly1), (lx2, ly2), (90, 255, 255), 6)\n",
                "            detections.append({\"center\": max(ly1, ly2), \"type\": \"basic_hough\"})\n",
                "\n",
                "    # Method B: Edge Density\n",
                "    densities = []\n",
                "    strip_h = config[\"strip_height\"]\n",
                "\n",
                "    for y in range(0, h - strip_h, 10):\n",
                "        strip = edges[y : y + strip_h, :]\n",
                "        density = np.sum(strip) / (strip_h * w)\n",
                "        densities.append((y + strip_h // 2, density))\n",
                "\n",
                "    dens_vals = [d[1] for d in densities]\n",
                "    if dens_vals:\n",
                "        threshold = np.mean(dens_vals) + config[\"density_threshold\"] * np.std(dens_vals)\n",
                "        for y, density in densities:\n",
                "            if density > threshold:\n",
                "                cv2.line(vis, (0, y), (w, y), (90, 255, 255), 6)\n",
                "                detections.append({\"center\": y, \"type\": \"edge_density\"})\n",
                "\n",
                "    if detections:\n",
                "        # The true top edge is the lowest (highest Y value) line found above the data\n",
                "        max_det = max(detections, key=lambda x: x[\"center\"])\n",
                "        return max(current_y1, max_det[\"center\"])\n",
                "\n",
                "    return current_y1\n",
                "\n",
                "\n",
                "def crop(image_path: Path, debug=True):\n",
                "    \"\"\"\n",
                "    Crops the image to the table region using a multi-stage approach:\n",
                "    1. PaddleOCR Layout to find the rough table bbox.\n",
                "    2. Text detection to expand the bbox if text protrudes.\n",
                "    3. Heuristic edge detection to refine the top header line.\n",
                "    \"\"\"\n",
                "    image_path = Path(image_path)\n",
                "    stem = image_path.stem\n",
                "    working_dir = image_path.parent / stem\n",
                "    working_dir.mkdir(exist_ok=True)\n",
                "    working_image = working_dir / image_path.name\n",
                "\n",
                "    source = working_image if working_image.exists() else image_path\n",
                "\n",
                "    layout_model = LayoutDetection(model_name=\"PP-DocLayout_plus-L\")\n",
                "    text_model = TextDetection(model_name=\"PP-OCRv5_server_det\")\n",
                "\n",
                "    img = cv2.imread(str(source))\n",
                "    vis = img.copy()\n",
                "    h, w = img.shape[:2]\n",
                "\n",
                "    # --- Step 1: Coarse Table Detection (Layout Analysis) ---\n",
                "    layout_detection_results = layout_model.predict(str(source), batch_size=1, layout_nms=True, threshold=0.2)\n",
                "\n",
                "    table_bbox = None\n",
                "\n",
                "    for res in layout_detection_results:\n",
                "        if \"boxes\" not in res:\n",
                "            continue\n",
                "        tables = [b for b in res[\"boxes\"] if b.get(\"label\") == \"table\"]\n",
                "        if tables:\n",
                "            # Pick highest confidence table\n",
                "            b = max(tables, key=lambda x: x.get(\"score\", 0))\n",
                "            c = b[\"coordinate\"]\n",
                "            table_bbox = [int(c[0]), int(c[1]), int(c[2]), int(c[3])]\n",
                "            break\n",
                "\n",
                "    if table_bbox is None:\n",
                "        try:\n",
                "            if source != working_image:\n",
                "                shutil.copy2(source, working_image)\n",
                "                if debug:\n",
                "                    print(f\"No table detected — copied original to {working_image}\")\n",
                "            else:\n",
                "                if debug:\n",
                "                    print(f\"No table detected — image already in working dir\")\n",
                "        except OSError as e:\n",
                "            print(f\"Copy failed for {stem}: {e}\")\n",
                "        return\n",
                "\n",
                "    x1, y1, x2, y2 = table_bbox\n",
                "    cv2.rectangle(vis, (x1, y1), (x2, y2), (255, 0, 0), 30)\n",
                "    gx1, gy1, gx2, gy2 = table_bbox  # Save original global detection\n",
                "\n",
                "    # --- Step 2: Expand Sides (Text Detection) ---\n",
                "    # If text boxes protrude from the layout bbox, expand to include them\n",
                "    text_detection_results = text_model.predict(str(source), batch_size=1)\n",
                "\n",
                "    for res in text_detection_results:\n",
                "        if \"dt_polys\" not in res:\n",
                "            continue\n",
                "        for poly in res[\"dt_polys\"]:\n",
                "            poly = np.asarray(poly)\n",
                "            px1, py1 = int(np.min(poly[:, 0])), int(np.min(poly[:, 1]))\n",
                "            px2, py2 = int(np.max(poly[:, 0])), int(np.max(poly[:, 1]))\n",
                "\n",
                "            # Check if text is vertically within the table region\n",
                "            inside_vertically = py1 >= gy1 and py2 <= gy2\n",
                "            if inside_vertically:\n",
                "                if px1 < gx1 and px2 > gx1:\n",
                "                    x1 = min(x1, px1)  # Expand Left\n",
                "                if px2 > gx2 and px1 < gx2:\n",
                "                    x2 = max(x2, px2)  # Expand Right\n",
                "\n",
                "            cv2.rectangle(vis, (px1, py1), (px2, py2), (92, 183, 206), 1)\n",
                "\n",
                "    # --- Step 3: Refine Top Edge (Line/Density Detection) ---\n",
                "    # Finds the specific line separating headers from data=\n",
                "    new_y1 = _find_table_top_edge(img, vis, LINE_DETECTION_CONFIG, y1)\n",
                "\n",
                "    if new_y1 > y1:\n",
                "        # Lowest edge line\n",
                "        cv2.line(vis, (0, new_y1), (w, new_y1), (0, 0, 255), 10)\n",
                "        y1 = new_y1\n",
                "\n",
                "    # --- Step 4: Final Crop & Save ---\n",
                "    x1, y1 = max(0, x1), max(0, y1)\n",
                "    x2, y2 = min(w, x2), min(h, y2)\n",
                "\n",
                "    if x2 > x1 and y2 > y1:\n",
                "        cropped = img[y1:y2, x1:x2]\n",
                "        cv2.imwrite(str(working_image), cropped)\n",
                "        cv2.rectangle(vis, (x1, y1), (x2, y2), (81, 255, 0), 20)  # Green\n",
                "\n",
                "        if debug:\n",
                "            cv2.imwrite(str(working_dir / f\"{stem}_crop.jpg\"), vis)\n",
                "            print(f\"Cropped: {working_image}\")\n",
                "\n",
                "\n",
                "crop(IMAGE_PATH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d29388f3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3: Text detection\n",
                "\n",
                "\n",
                "reader = easyocr.Reader([\"en\"], gpu=has_gpu)\n",
                "\n",
                "# Parameters are found using 'find_text_bbox.py'\n",
                "TEXT_BBOX_CONFIG = {\n",
                "    \"threshold\": 125,\n",
                "    \"kernel_size\": (5, 5),\n",
                "    \"iterations\": 2,\n",
                "}\n",
                "\n",
                "\n",
                "def has_overlap(ocv_bbox: list[int], easyocr_bboxes: list[list[int]]) -> bool:\n",
                "    for eb in easyocr_bboxes:\n",
                "        if ocv_bbox[0] < eb[2] and ocv_bbox[2] > eb[0] and ocv_bbox[1] < eb[3] and ocv_bbox[3] > eb[1]:\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "\n",
                "def detect_text_bboxes_opencv(image: np.ndarray, config: dict) -> list[list[int]]:\n",
                "    \"\"\"\n",
                "    Generates precise text bounding boxes by intersecting OpenCV morphological\n",
                "    candidates with EasyOCR detections to filter noise.\n",
                "    \"\"\"\n",
                "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
                "    _, thresh = cv2.threshold(gray, config[\"threshold\"], 255, cv2.THRESH_BINARY_INV)\n",
                "    # Note: numpy shape is (height, width), so kernel_size[1] then [0]\n",
                "    kernel = np.ones((config[\"kernel_size\"][1], config[\"kernel_size\"][0]), np.uint8)\n",
                "    dilated = cv2.dilate(thresh, kernel, iterations=config[\"iterations\"])\n",
                "\n",
                "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "\n",
                "    bboxes = []\n",
                "    for contour in contours:\n",
                "        x, y, w, h = cv2.boundingRect(contour)\n",
                "        bboxes.append([x, y, x + w, y + h])\n",
                "\n",
                "    return bboxes\n",
                "\n",
                "\n",
                "def detect_text_bboxes_easyocr(reader: easyocr.Reader, working_image: Path, height: int, width: int) -> list[list[int]]:\n",
                "    horizontal_list, free_list = reader.detect(\n",
                "        str(working_image),\n",
                "        min_size=1,\n",
                "        text_threshold=0.3,\n",
                "        low_text=0.3,\n",
                "        link_threshold=0.05,\n",
                "        width_ths=0.1,\n",
                "        canvas_size=max(height, width),\n",
                "        mag_ratio=1.5,\n",
                "    )\n",
                "\n",
                "    bboxes = []\n",
                "    for x_min, x_max, y_min, y_max in horizontal_list[0]:\n",
                "        bboxes.append([int(x_min), int(y_min), int(x_max), int(y_max)])\n",
                "\n",
                "    for points in free_list[0]:\n",
                "        points = np.array(points)\n",
                "        x1, y1 = int(np.min(points[:, 0])), int(np.min(points[:, 1]))\n",
                "        x2, y2 = int(np.max(points[:, 0])), int(np.max(points[:, 1]))\n",
                "        bboxes.append([x1, y1, x2, y2])\n",
                "\n",
                "    return bboxes\n",
                "\n",
                "\n",
                "def detect_text_bboxes(\n",
                "    image_path: Path, reader: easyocr.Reader, config: dict, debug_easyocr: bool = False\n",
                ") -> list[list[int]]:\n",
                "    \"\"\"\n",
                "    Detects precise text bounding boxes by filtering OpenCV candidates with EasyOCR.\n",
                "\n",
                "    The logic is:\n",
                "    1. Generate tight, precise bounding boxes using OpenCV morphological operations.\n",
                "    2. Generate rough text regions using EasyOCR.\n",
                "    3. Keep OpenCV boxes ONLY if they overlap with an EasyOCR region.\n",
                "\n",
                "    This combines OpenCV's precision with EasyOCR's ability to ignore\n",
                "    non-text artifacts (like binding holes or scan artifacts).\n",
                "    \"\"\"\n",
                "    image_path = Path(image_path)\n",
                "    stem = image_path.stem\n",
                "    working_dir = image_path.parent / stem\n",
                "    working_image = working_dir / image_path.name\n",
                "\n",
                "    image = cv2.imread(str(working_image))\n",
                "    height, width = image.shape[:2]\n",
                "\n",
                "    easyocr_bboxes = detect_text_bboxes_easyocr(reader, working_image, height, width)\n",
                "    opencv_bboxes = detect_text_bboxes_opencv(image, config)\n",
                "\n",
                "    if debug_easyocr:\n",
                "        debug_vis = image.copy()\n",
                "        for x1, y1, x2, y2 in easyocr_bboxes:\n",
                "            cv2.rectangle(debug_vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
                "        cv2.imwrite(str(working_dir / f\"{stem}_easyocr_bbox.jpg\"), debug_vis)\n",
                "\n",
                "    filtered_bboxes = [b for b in opencv_bboxes if has_overlap(b, easyocr_bboxes)]\n",
                "\n",
                "    # Keeps numbering consistent (Top-to-Bottom, Left-to-Right)\n",
                "    filtered_bboxes.sort(key=lambda b: (b[1], b[0]))\n",
                "\n",
                "    vis = image.copy()\n",
                "    for x1, y1, x2, y2 in filtered_bboxes:\n",
                "        cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
                "\n",
                "    output_path = working_dir / f\"{stem}_txt_bbox.jpg\"\n",
                "    cv2.imwrite(str(output_path), vis)\n",
                "\n",
                "    with (working_dir / f\"{stem}_bbox.json\").open(\"w\") as f:\n",
                "        json.dump(filtered_bboxes, f, indent=2)\n",
                "\n",
                "    return filtered_bboxes\n",
                "\n",
                "\n",
                "bboxes = detect_text_bboxes(IMAGE_PATH, reader, TEXT_BBOX_CONFIG, debug_easyocr=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "55bc5aeb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4: Detect row/column lines, assign text bboxes to cells and store all data for future use\n",
                "\n",
                "# Parameters are found using 'find_peak_intensity.py'\n",
                "PEAK_CONFIG = {\n",
                "    \"denoise_strength\": 10,\n",
                "    \"threshold\": 234,\n",
                "    \"row_min_height\": 103,\n",
                "    \"row_min_distance\": 58,\n",
                "    \"row_prominence\": 106,\n",
                "    \"col_min_height\": 34,\n",
                "    \"col_min_distance\": 34,\n",
                "    \"col_prominence\": 89,\n",
                "}\n",
                "\n",
                "\n",
                "def detect_separators(image_path: Path, bbox_data: list[list[int]], config: dict) -> dict[str, Any]:\n",
                "    image = cv2.imread(str(image_path))\n",
                "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
                "\n",
                "    # Use config for denoising strength (h) and thresholding\n",
                "    gray = cv2.fastNlMeansDenoising(gray, None, config[\"denoise_strength\"], 7, 21)\n",
                "    _, gray = cv2.threshold(gray, config[\"threshold\"], 255, cv2.THRESH_BINARY)\n",
                "\n",
                "    height, width = gray.shape\n",
                "\n",
                "    # Detect Row Separators\n",
                "    h_projection = np.mean(255 - gray, axis=1)\n",
                "    h_peaks, _ = find_peaks(\n",
                "        h_projection,\n",
                "        height=config[\"row_min_height\"],\n",
                "        distance=config[\"row_min_distance\"],\n",
                "        prominence=config[\"row_prominence\"],\n",
                "    )\n",
                "    row_separators = sorted(list(set([0] + h_peaks.tolist() + [height])))\n",
                "\n",
                "    # Detect Column Separators\n",
                "    v_projection = np.mean(255 - gray, axis=0)\n",
                "    v_peaks, _ = find_peaks(\n",
                "        v_projection,\n",
                "        height=config[\"col_min_height\"],\n",
                "        distance=config[\"col_min_distance\"],\n",
                "        prominence=config[\"col_prominence\"],\n",
                "    )\n",
                "    col_separators = v_peaks.tolist()\n",
                "\n",
                "    # Ensure column separators encompass all text\n",
                "    if bbox_data:\n",
                "        all_x_coords = [coord for bbox in bbox_data for coord in (bbox[0], bbox[2])]\n",
                "        # Guard clause in case bbox_data is empty\n",
                "        if all_x_coords:\n",
                "            min_text_x = min(all_x_coords)\n",
                "            max_text_x = max(all_x_coords)\n",
                "\n",
                "            if not col_separators or min_text_x < col_separators[0]:\n",
                "                col_separators.insert(0, 0)\n",
                "\n",
                "            if not col_separators or max_text_x > col_separators[-1]:\n",
                "                col_separators.append(width)\n",
                "\n",
                "    col_separators = sorted(list(set(col_separators)))\n",
                "\n",
                "    if len(col_separators) < 2:\n",
                "        col_separators = [0, width]\n",
                "\n",
                "    return {\n",
                "        \"image_dimensions\": {\"width\": width, \"height\": height},\n",
                "        \"row_separators\": row_separators,\n",
                "        \"column_separators\": col_separators,\n",
                "    }\n",
                "\n",
                "\n",
                "def assign_text_to_cells(separators: dict[str, Any], bbox_data: list[list[int]]) -> dict[str, dict[str, Any]]:\n",
                "    row_seps = separators[\"row_separators\"]\n",
                "    col_seps = separators[\"column_separators\"]\n",
                "\n",
                "    grid = {}\n",
                "    logical_row = 0\n",
                "\n",
                "    for r in range(len(row_seps) - 1):\n",
                "        top = row_seps[r]\n",
                "        bottom = row_seps[r + 1]\n",
                "\n",
                "        # Check if row contains any text\n",
                "        row_bboxes = [i for i, (x1, y1, x2, y2) in enumerate(bbox_data) if top <= (y1 + y2) / 2 <= bottom]\n",
                "        if not row_bboxes:\n",
                "            continue\n",
                "\n",
                "        for c in range(len(col_seps) - 1):\n",
                "            cell_key = f\"{logical_row},{c}\"\n",
                "            grid[cell_key] = {\n",
                "                \"row\": logical_row,\n",
                "                \"column\": c,\n",
                "                \"bounds\": {\n",
                "                    \"top\": top,\n",
                "                    \"bottom\": bottom,\n",
                "                    \"left\": col_seps[c],\n",
                "                    \"right\": col_seps[c + 1],\n",
                "                },\n",
                "                \"text_elements\": [],\n",
                "            }\n",
                "\n",
                "            # Assign specific text boxes to this cell\n",
                "            for i, coords in enumerate(bbox_data):\n",
                "                x1, y1, x2, y2 = coords\n",
                "                cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
                "\n",
                "                # Check if center of text box is within cell bounds\n",
                "                if col_seps[c] <= cx <= col_seps[c + 1] and top <= cy <= bottom:\n",
                "                    grid[cell_key][\"text_elements\"].append(\n",
                "                        {\"bbox\": {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2}, \"index\": i}\n",
                "                    )\n",
                "\n",
                "        logical_row += 1\n",
                "\n",
                "    return grid\n",
                "\n",
                "\n",
                "def create_table_visualization(image_path: Path, result: dict[str, Any]) -> np.ndarray:\n",
                "    image = cv2.imread(str(image_path))\n",
                "    vis_image = image.copy()\n",
                "\n",
                "    separators = result[\"separators\"]\n",
                "    height, width = vis_image.shape[:2]\n",
                "\n",
                "    # Draw Separators\n",
                "    for y in separators[\"row_separators\"]:\n",
                "        cv2.line(vis_image, (0, y), (width, y), (0, 120, 255), 2)\n",
                "\n",
                "    for x in separators[\"column_separators\"]:\n",
                "        cv2.line(vis_image, (x, 0), (x, height), (0, 255, 0), 2)\n",
                "\n",
                "    # Draw Cells and Indices\n",
                "    grid = result[\"table_grid\"]\n",
                "    for cell_key, cell in grid.items():\n",
                "        row, col = cell[\"row\"], cell[\"column\"]\n",
                "        bounds = cell[\"bounds\"]\n",
                "\n",
                "        offset = 10\n",
                "\n",
                "        text_x = bounds[\"left\"] + offset\n",
                "        text_y = bounds[\"top\"] + 20\n",
                "\n",
                "        cv2.putText(\n",
                "            vis_image,\n",
                "            f\"({row},{col})\",\n",
                "            (text_x, text_y),\n",
                "            cv2.FONT_HERSHEY_SIMPLEX,\n",
                "            0.4,\n",
                "            (255, 0, 0),\n",
                "            1,\n",
                "        )\n",
                "\n",
                "        for text_elem in cell[\"text_elements\"]:\n",
                "            bbox = text_elem[\"bbox\"]\n",
                "            x1, y1, x2, y2 = bbox[\"x1\"], bbox[\"y1\"], bbox[\"x2\"], bbox[\"y2\"]\n",
                "\n",
                "            cv2.rectangle(vis_image, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
                "            cv2.putText(vis_image, str(text_elem[\"index\"]), (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
                "\n",
                "    return vis_image\n",
                "\n",
                "\n",
                "def detect_table_structure(image_path: Path, config: dict) -> None:\n",
                "    image_path = Path(image_path)\n",
                "    stem = image_path.stem\n",
                "    working_dir = image_path.parent / stem\n",
                "    working_image = working_dir / image_path.name\n",
                "    bbox_json = working_dir / f\"{stem}_bbox.json\"\n",
                "\n",
                "    bbox_data = []\n",
                "    if bbox_json.exists():\n",
                "        with bbox_json.open(\"r\") as f:\n",
                "            bbox_data = json.load(f)\n",
                "\n",
                "    # Pass the config dictionary directly\n",
                "    separators = detect_separators(working_image, bbox_data, config)\n",
                "\n",
                "    grid = assign_text_to_cells(separators, bbox_data)\n",
                "    result = {\"image_name\": stem, \"separators\": separators, \"table_grid\": grid}\n",
                "\n",
                "    with (working_dir / f\"{stem}_table.json\").open(\"w\") as f:\n",
                "        json.dump(result, f, indent=2)\n",
                "\n",
                "    vis_image = create_table_visualization(working_image, result)\n",
                "    cv2.imwrite(str(working_dir / f\"{stem}_table.jpg\"), vis_image)\n",
                "\n",
                "\n",
                "detect_table_structure(IMAGE_PATH, PEAK_CONFIG)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0e80c504",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5: Segment text by row, reordering columns within each segment\n",
                "\n",
                "SEGMENT_PADDING = 65  # Pads the ends of a segment (px)\n",
                "COLUMN_GAP = 30  # Spacing between column text (px)\n",
                "\n",
                "\n",
                "def crop_text_from_column(image: np.ndarray, bboxes: list[dict[str, int]]) -> dict[str, Any] | None:\n",
                "    \"\"\"Extract and combine all text bboxes from a single column into one image.\"\"\"\n",
                "    if not bboxes:\n",
                "        return None\n",
                "\n",
                "    min_x = min(bbox[\"x1\"] for bbox in bboxes)\n",
                "    max_x = max(bbox[\"x2\"] for bbox in bboxes)\n",
                "    min_y = min(bbox[\"y1\"] for bbox in bboxes)\n",
                "    max_y = max(bbox[\"y2\"] for bbox in bboxes)\n",
                "\n",
                "    width = max_x - min_x\n",
                "    height = max_y - min_y\n",
                "\n",
                "    result = np.full((height, width, 3), 255, dtype=np.uint8)\n",
                "\n",
                "    for bbox in bboxes:\n",
                "        x1, y1, x2, y2 = bbox[\"x1\"], bbox[\"y1\"], bbox[\"x2\"], bbox[\"y2\"]\n",
                "        bbox_crop = image[y1:y2, x1:x2]\n",
                "\n",
                "        if bbox_crop.size == 0:\n",
                "            continue\n",
                "\n",
                "        rel_x = x1 - min_x\n",
                "        rel_y = y1 - min_y\n",
                "        result[rel_y : rel_y + bbox_crop.shape[0], rel_x : rel_x + bbox_crop.shape[1]] = bbox_crop\n",
                "\n",
                "    return {\"image\": result, \"width\": width, \"height\": height}\n",
                "\n",
                "\n",
                "def assemble_segment(image: np.ndarray, bboxes: list[dict[str, Any]], column_order: list[int]) -> np.ndarray:\n",
                "    \"\"\"Combine multiple columns into a single segment image, ordered by column_order.\"\"\"\n",
                "    if not bboxes:\n",
                "        return np.full((100, 100, 3), 255, dtype=np.uint8)\n",
                "\n",
                "    columns = []\n",
                "    max_height = 0\n",
                "\n",
                "    for col_idx in column_order:\n",
                "        col_bboxes = [bbox for bbox in bboxes if bbox[\"column\"] == col_idx]\n",
                "        if col_bboxes:\n",
                "            col_content = crop_text_from_column(image, col_bboxes)\n",
                "            if col_content:\n",
                "                columns.append(col_content)\n",
                "                max_height = max(max_height, col_content[\"height\"])\n",
                "\n",
                "    if not columns:\n",
                "        return np.full((100, 100, 3), 255, dtype=np.uint8)\n",
                "\n",
                "    total_segment_width = SEGMENT_PADDING\n",
                "    for i, col in enumerate(columns):\n",
                "        total_segment_width += col[\"width\"]\n",
                "        if i < len(columns) - 1:\n",
                "            total_segment_width += COLUMN_GAP\n",
                "    total_segment_width += SEGMENT_PADDING\n",
                "\n",
                "    result = np.full((max_height, total_segment_width, 3), 255, dtype=np.uint8)\n",
                "\n",
                "    x_position = SEGMENT_PADDING\n",
                "    for i, col in enumerate(columns):\n",
                "        y_offset = (max_height - col[\"height\"]) // 2\n",
                "        result[y_offset : y_offset + col[\"height\"], x_position : x_position + col[\"width\"]] = col[\"image\"]\n",
                "\n",
                "        x_position += col[\"width\"]\n",
                "        if i < len(columns) - 1:\n",
                "            x_position += COLUMN_GAP\n",
                "\n",
                "    return result\n",
                "\n",
                "\n",
                "def group_cells_by_row(grid: dict[str, dict]) -> dict[str, list[dict]]:\n",
                "    \"\"\"Group cell data by row index.\"\"\"\n",
                "    rows = defaultdict(list)\n",
                "    for cell_key, cell_data in grid.items():\n",
                "        row_idx = cell_key.split(\",\")[0]\n",
                "        rows[row_idx].append({\"cell_key\": cell_key, \"cell_data\": cell_data})\n",
                "    return dict(rows)\n",
                "\n",
                "\n",
                "def pad_to_height(segment: np.ndarray, target_height: int) -> np.ndarray:\n",
                "    \"\"\"Center segment vertically within target height.\"\"\"\n",
                "    current_height = segment.shape[0]\n",
                "    if current_height >= target_height:\n",
                "        return segment[:target_height]\n",
                "\n",
                "    result = np.full((target_height, segment.shape[1], 3), 255, dtype=np.uint8)\n",
                "    y_offset = (target_height - current_height) // 2\n",
                "    result[y_offset : y_offset + current_height] = segment\n",
                "    return result\n",
                "\n",
                "\n",
                "def create_row_segments(\n",
                "    image: np.ndarray, row_cells: list[dict], column_groups: list[list[int]]\n",
                ") -> list[np.ndarray | None]:\n",
                "    \"\"\"Create segment images for a single row, one per column group.\"\"\"\n",
                "    all_bboxes = []\n",
                "    for cell_info in row_cells:\n",
                "        cell_data = cell_info[\"cell_data\"]\n",
                "        for text_elem in cell_data.get(\"text_elements\", []):\n",
                "            bbox = text_elem[\"bbox\"]\n",
                "            all_bboxes.append(\n",
                "                {\n",
                "                    \"x1\": bbox[\"x1\"],\n",
                "                    \"y1\": bbox[\"y1\"],\n",
                "                    \"x2\": bbox[\"x2\"],\n",
                "                    \"y2\": bbox[\"y2\"],\n",
                "                    \"column\": cell_data[\"column\"],\n",
                "                }\n",
                "            )\n",
                "\n",
                "    if not all_bboxes:\n",
                "        return [None] * len(column_groups)\n",
                "\n",
                "    segments = []\n",
                "    for column_group in column_groups:\n",
                "        group_bboxes = [bbox for bbox in all_bboxes if bbox[\"column\"] in column_group]\n",
                "        if group_bboxes:\n",
                "            assembled = assemble_segment(image, group_bboxes, column_group)\n",
                "            # Fixed px height of segments\n",
                "            final = pad_to_height(assembled, 100)\n",
                "            segments.append(final)\n",
                "        else:\n",
                "            segments.append(None)\n",
                "\n",
                "    return segments\n",
                "\n",
                "\n",
                "def segment_text_by_row(image_path: Path, column_groups: list[list[int]]) -> list[str]:\n",
                "    \"\"\"\n",
                "    Segments the image into row strips based on the detected table grid.\n",
                "\n",
                "    It reorders and groups columns within each row based on the 'column_groups'\n",
                "    configuration, allowing you to restructure the table layout during extraction.\n",
                "\n",
                "    Args:\n",
                "        image_path: Path to the original image.\n",
                "        column_groups: A list of lists defining the output order and grouping.\n",
                "        Ex: [[0, 1], [2]] will create two images per row:\n",
                "        one combining cols 0 & 1, and a second image containing only col 2.\n",
                "\n",
                "    Returns:\n",
                "        List of paths to the generated segment images.\n",
                "    \"\"\"\n",
                "    image_path = Path(image_path)\n",
                "    stem = image_path.stem\n",
                "    working_dir = image_path.parent / stem\n",
                "    working_image = working_dir / image_path.name\n",
                "    table_json = working_dir / f\"{stem}_table.json\"\n",
                "    output_dir = working_dir / \"rows\"\n",
                "\n",
                "    image = cv2.imread(str(working_image))\n",
                "\n",
                "    with table_json.open(\"r\") as f:\n",
                "        table_data = json.load(f)\n",
                "\n",
                "    grid = table_data[\"table_grid\"]\n",
                "    output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "    rows = group_cells_by_row(grid)\n",
                "    output_paths = []\n",
                "\n",
                "    sorted_row_ids = sorted(rows.keys(), key=lambda x: int(x))\n",
                "\n",
                "    for row_id in sorted_row_ids:\n",
                "        row_cells = rows[row_id]\n",
                "        segments = create_row_segments(image, row_cells, column_groups)\n",
                "\n",
                "        for seg_idx, segment in enumerate(segments, 1):\n",
                "            if segment is not None:\n",
                "                filename = f\"{stem}_{row_id}-{seg_idx}.jpg\"\n",
                "                output_path = output_dir / filename\n",
                "                cv2.imwrite(str(output_path), segment)\n",
                "                output_paths.append(str(output_path))\n",
                "\n",
                "    print(f\"Segmented {len(output_paths)} row images\")\n",
                "    return output_paths\n",
                "\n",
                "\n",
                "column_groups = [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n",
                "# column_groups = [[13], [12], [11], [10], [9], [8], [7], [6], [5], [4], [3], [2], [1], [0]]\n",
                "# column_groups = [[8, 3, 1, 4], [13, 10,11, 0, 9], [5, 2, 6, 7, 12]]\n",
                "segment_text_by_row(IMAGE_PATH, column_groups)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
