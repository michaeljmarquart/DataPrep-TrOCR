{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers pinned to 4.44.2 - later versions have breaking changes with TrOCR\n",
    "# See: https://discuss.huggingface.co/t/fine-tune-trocr-model/151014\n",
    "%pip install accelerate jiwer tensorboard transformers==4.44.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    EarlyStoppingCallback,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrOCRProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    ")\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "\n",
    "from utils import RandomNoise, shift_tokens_right\n",
    "\n",
    "seed_value = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"microsoft/trocr-base-printed\"\n",
    "\n",
    "image_dir = project_root / \"data\" / \"images\"\n",
    "csv_path = project_root / \"data\" / \"ground_truths.csv\"\n",
    "\n",
    "output_dir = project_root / \"output\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(f\"Could not find CSV at: {csv_path}.\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Loaded {len(df)} records from CSV\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(base_model)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(base_model)\n",
    "\n",
    "# Configure model tokens\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "# Dropout settings\n",
    "model.decoder.config.dropout = 0.01\n",
    "model.decoder.config.attention_dropout = 0.01\n",
    "\n",
    "# Dynamic max length calculation based on dataset\n",
    "token_lengths = df[\"text\"].apply(lambda x: len(processor.tokenizer(x).input_ids))\n",
    "max_target_length = int((token_lengths.max() + 7) // 8 * 8)\n",
    "model.generation_config.max_length = max_target_length\n",
    "\n",
    "print(f\"Max target length set to: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100caa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pathing to images\n",
    "df[\"image_path\"] = df[\"filename\"].apply(lambda x: str(Path(image_dir) / x))\n",
    "dataset = Dataset.from_pandas(df[[\"image_path\", \"text\"]])\n",
    "train_test = dataset.train_test_split(test_size=0.10, seed=seed_value)\n",
    "train_ds = train_test[\"train\"]\n",
    "eval_ds = train_test[\"test\"]\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(degrees=10, fill=(255, 255, 255)),\n",
    "        transforms.RandomAffine(degrees=0, shear=5, fill=(255, 255, 255)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        RandomNoise(prob=0.25),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def process_data(batch, is_train=True):\n",
    "    \"\"\"\n",
    "    Prepares batch for TrOCR:\n",
    "    1. Applies augmentations to images (if training).\n",
    "    2. Tokenizes text and masks padding tokens (-100) for loss calculation.\n",
    "    3. Creates shifted decoder_input_ids for auto-regressive generation.\n",
    "    \"\"\"\n",
    "    images = [\n",
    "        (train_transform(Image.open(p).convert(\"RGB\")) if is_train else Image.open(p).convert(\"RGB\"))\n",
    "        for p in batch[\"image_path\"]\n",
    "    ]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    encoding = processor.tokenizer(\n",
    "        batch[\"text\"], padding=\"max_length\", truncation=True, max_length=max_target_length, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels = encoding.input_ids.clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # shift_tokens_right imported from utils.py\n",
    "    decoder_input_ids = shift_tokens_right(\n",
    "        encoding.input_ids, processor.tokenizer.pad_token_id, model.config.decoder_start_token_id\n",
    "    )\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"decoder_input_ids\": decoder_input_ids, \"labels\": labels}\n",
    "\n",
    "\n",
    "# Apply processing to datasets\n",
    "train_ds = train_ds.map(\n",
    "    lambda b: process_data(b, True), batched=True, batch_size=8, remove_columns=train_ds.column_names\n",
    ")\n",
    "eval_ds = eval_ds.map(lambda b: process_data(b, False), batched=True, batch_size=8, remove_columns=eval_ds.column_names)\n",
    "\n",
    "train_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"decoder_input_ids\", \"labels\"])\n",
    "eval_ds.set_format(type=\"torch\", columns=[\"pixel_values\", \"decoder_input_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5825ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "exact_match_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    exact_match = exact_match_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    error_count = 0\n",
    "\n",
    "    with open(output_dir / \"eval_errors.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"\\n{'='*20} EVAL {'='*20}\\n\")\n",
    "        for p, l in zip(pred_str, label_str):\n",
    "            if p != l:\n",
    "                error_count += 1\n",
    "                f.write(f\"TRUTH: {l}\\nPRED : {p}\\n{'-'*30}\\n\")\n",
    "\n",
    "    if error_count > 0:\n",
    "        print(f\"Logged {len(error_count)} errors to eval_errors.txt\")\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer, \"exact_match\": exact_match[\"exact_match\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66305bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollator:\n",
    "    processor: TrOCRProcessor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        return {\n",
    "            \"pixel_values\": torch.stack([f[\"pixel_values\"] for f in features]),\n",
    "            \"decoder_input_ids\": torch.stack([f[\"decoder_input_ids\"] for f in features]),\n",
    "            \"labels\": torch.stack([f[\"labels\"] for f in features]),\n",
    "        }\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(output_dir),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=8,  # Effective batch size: 4 * 8 = 32\n",
    "    learning_rate=5e-6,  # Conservative LR for fine-tuning pretrained model\n",
    "    warmup_steps=200,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.03,\n",
    "    max_grad_norm=1.0,\n",
    "    fp16=True,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    generation_num_beams=4,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"exact_match\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=str(output_dir / \"logs\"),\n",
    "    logging_steps=25,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    disable_tqdm=False,\n",
    "    seed=seed_value,\n",
    "    num_train_epochs=10.0,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=DataCollator(processor),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90055495",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(output_dir / \"model\")\n",
    "processor.save_pretrained(output_dir / \"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
